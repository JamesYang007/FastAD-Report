\subsection{Log-Sum-Exp}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/log_sum_exp_fig.png}
    \caption{%
        Log-sum-exp benchmark of other libraries against FastAD 
        plotted relative to FastAD average time.
    }\label{fig:log_sum_exp}
\end{figure*}

The log-sum-exp function is defined as $f:\R^n \to \R$ 
where~$f(x) = \log(\sum\limits_{i=1}^n e^{x_i})$,
The only library supporting a built-in function for log-sum-exp is Stan.
Fig.\ref{fig:log_sum_exp} shows the benchmark results.

FastAD outperforms all libraries for all values of $N$.
The next three fastest libraries are Adept, Stan, and Sacado, respectively.
The trend stabilizes starting from $N=2^6=64$ where 
Adept is about $ 3$ times slower than FastAD, 
Stan about $ 5$ times, and 
Sacado about $ 17$ times.
Note that FastAD is only marginally slower than the baseline
despite calls to expensive functions like \code{log} and \code{exp}.
In fact, FastAD is only $ 1.5$ times slower than the baseline, 
meaning there is only about $ 50\%$
overhead from one forward-evaluation to also compute the gradient.
This is not surprising since FastAD reuses
the forward-evaluated result for \code{exp} expression
during the backward-evaluation (see Section~\ref{ssec:unary}).

\section{Benchmarks}\label{sec:benchmark}

In this section, we compare performances of 
various libraries against FastAD for a range of examples\footnotemark.
\footnotetext{github page: https://github.com/JamesYang007/ADBenchmark}
The following is an alphabetical list of the libraries used for benchmarking:\@
\begin{itemize}
    \item \href{http://www.met.reading.ac.uk/clouds/adept/}{Adept 2.0.8}~\cite{hogan:2014}
    \item \href{https://github.com/coin-or/ADOL-C}{ADOL-C 2.7.2}~\cite{griewank:1996}
    \item \href{https://coin-or.github.io/CppAD/doc/cppad.htm}{CppAD 20200000}~\cite{bell:2020}
    \item \href{https://github.com/JamesYang007/FastAD}{FastAD 3.1.0}
    \item \href{https://github.com/trilinos/Trilinos/tree/master/packages/sacado}{Sacado 13.0.0}~\cite{phipps:2009}
    \item \href{https://github.com/stan-dev/math}{Stan Math Library 3.3.0}~\cite{carpenter:2015}
\end{itemize}
All the libraries are at their most recent release at the time of benchmarking.
These libraries have also been used by others~\cite{carpenter:2015}\cite{margossian:2018}\cite{hogan:2014}.

All benchmarks were run on a Google Cloud Virtual Machine with the following configuration:
\begin{itemize}
    \item \textbf{OS}: Ubuntu 18.04.1 
    \item \textbf{Architecture}: x86 64-bit
    \item \textbf{Processor}: Intel Xeon Processor 
    \item \textbf{Cores}: 8
    \item \textbf{Compiler}: GCC10
    \item \textbf{C++ Standard}: 17
    \item \textbf{Compiler Optimization Flags}: \code{-O3 -march=native} 
\end{itemize}

All benchmarks benchmark the case where a user wishes to differentiate
a scalar function $f$ for different values of $x$.
This is a very common use-case.
For example, in the Newton-Raphson method,
we have to compute $f'(x_n)$ at every iteration with the updated $x_n$ value.
In HMC and NUTS, the leapfrog algorithm frequently
updates a quantity called the ``momentum vector'' $\rho$ 
with $\nabla_\theta \log(p(\theta, x))$ ($x$ here is treated as a constant),
where $\theta$ is a ``position vector'' that also gets frequently updated.

Our benchmark drivers are very similar to the ones used by Stan~\cite{carpenter:2015}.
As noted in~\cite{margossian:2018}, there is no standard set of benchmark examples for AD,
but since Stan is most similar to FastAD in both design and intended usage,
we wanted to keep the benchmark as similar as possible.

We measure the average time to differentiate a function
with an initial input and 
save the function evaluation result as well as the gradient.
After timing each library, the gradients are compared with manually-written gradient computation to check accuracy.
All libraries had some numerical issues for some of the examples,
but the maximum proportion of error to the actual gradient values was on the order of $ 10^{-15}$, which is negligible.
Hence, in terms of accuracy, all libraries were equally acceptable.
Finally, all benchmarks were written in the most optimal way for every library based on their documentation.

Every benchmark also times the ``baseline'', 
which is a manually-written forward evaluation (computing function value).
This will serve as a way to measure the extra overhead of computing the gradient relative to computing the function value.
This approach was also used in~\cite{carpenter:2015},
however in our benchmark, the baseline is also optimized to be vectorized.
Hence, our results for all libraries with respect to the baseline may differ from those in the reference.

Sections~\ref{ssec:sum_prod}-\ref{ssec:normal_log_pdf} 
cover some micro-benchmarks where we benchmark commonly used functions: 
summation and product of elements in a vector, 
log-sum-exponential, 
matrix multiplication, 
and normal log-pdf.
Sections~\ref{ssec:regression}-\ref{ssec:stochastic_volatility} 
cover some macro-benchmarks where we benchmark practical examples: 
a regression model and a stochastic volatility model.

\input{benchmark/sum_prod}

\input{benchmark/log_sum_exp}

\input{benchmark/matrix_mult}

\input{benchmark/normal_log_pdf}

\input{benchmark/regression}

\input{benchmark/stochastic_volatility}

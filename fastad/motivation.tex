\subsection{Motivation}\label{ssec:motivation}

The main motivations for our design are to employ vectorization, lazy-evaluation, and lazy-allocation.
Each of these topics are further elaborated below.
As it turns out, expression template is the perfect tool that achieves
all of these strategies.
Expression template is a template metaprogramming technique that 
uses type information, and often times operator overloads, to 
construct structures that represent complex expressions at compile-time.
Typically, it is used to represent a computation tree to perform lazy evaluation.
Section~\ref{ssec:example} briefly mentioned how expression templates allow for
lazy allocation.
As for vectorization, the goal is to reuse a well-polished and efficient
matrix library such as \code{Eigen} to create vectorized code.
Others have noted that integrating another expression-template based matrix library 
such as \code{Eigen} into an AD system can be quite challenging~\cite{hogan:2014}
and lead to unexpected problems that require extra memory allocations to resolve~\cite{carpenter:2015}.
However, once the AD system itself is fully based on expression-templates,
one can easily integrate any such matrix library.
For a full treatment of expression templates, 
we direct the reader to~\cite{vandevoorde:2002}.

\subsubsection{Vectorization}

Vectorization refers to the parallelization of operations on multiple data at the hardware level 
using specialized instructions called Single Instruction Multiple Data (SIMD) instructions.
On a modern Intel 64-bit processor supporting AVX, one of the modern SIMD instruction sets,
four double-precision floating point numbers can be processed simultaneously,
roughly improving performance by a factor of four.
While the compiler optimization is able to vectorize a user's code sometimes, it is not guaranteed
because vectorization requirements are quite stringent. 
For example, vectorization is not guaranteed if memory access is not done in a contiguous fashion
and is impossible if there is any dependency between loop iterations.
This makes it quite challenging to design an AD system that 
can always predict compiler optimization to create vectorized code.
However, vectorization can make AD extremely fast, powerful, and practical even in complex problems.
In practice, we come across many examples where operations can be vectorized during gradient computation.
For example, matrix multiplication, any reduction from a multi-dimensional variable to a scalar such as
summation or product of all elements, and any unary and binary function that is applied element-wise such as
exponential, logarithm, power, sin, cos, tan, and the usual arithmetic operators.

\subsubsection{Lazy Evaluation}\label{sssec:lazy-eval}

Lazy evaluation refers to the design pattern of delaying any computation until the moment that it is needed.
Often times, this method is used to reduce the number of temporary variables during the computation.
The prototypical example is a matrix library such as \code{Eigen}~\cite{eigen:2010}.
If an operation between two matrices always returned another matrix, 
this would easily cause both memory thrashing and fragmentation.
Instead, it is more prudent to return a cheap object that only remembers \emph{how} to evaluate the computation,
and only when the evaluation needs to occur, one-time allocate a matrix object, 
evaluate the computation, and store the result in that object.
AD evaluations will inevitably require matrix operations if we are to create a system
general enough to differentiate with respect to vectors or matrices,
and so this reduction of temporaries become useful.

Lazy evaluation can also be used during forward and backward-evaluation.
The reason for delaying forward-evaluation is to separate
the memory-related steps from the actual forward-evaluation step,
which are usually combined into one step in other libraries.
This leads to the idea of lazy allocation, which is discussed in the next section.
As for backward-evaluation, one can lazily compute the next seed for a given node's children.
After forward-evaluating node $w$, 
one could eagerly compute and save $\frac{\partial w}{\partial v}$
in Eq.~\ref{eq:next-seed} and during the backward-evaluation 
compute $\frac{\partial f}{\partial w} \frac{\partial w}{\partial v}$,
however, this is less efficient than 
computing $\frac{\partial f}{\partial w} \frac{\partial w}{\partial v}$ as a whole
during backward-evaluation,
since unnecessary computations may be carried out
and the product can usually be combined into fewer steps~\cite{carpenter:2015}.

\subsubsection{Lazy Allocation}\label{sssec:lazy-alloc}

We can take lazy evaluation even further in the context of AD to employ lazy allocation.
Similar to lazy evaluation, lazy allocation delays any \emph{memory allocations}.
Based on the algorithm described in Section~\ref{sec:reverse},
we have no choice but to save the intermediate values and adjoints for all expression nodes $w$
because $\frac{\partial f}{\partial w}$ is, in general, a function of $w$ (see Eq.~\ref{eq:next-seed})
and is only computed during backward-evaluation after \emph{all} forward-evaluations are completed.
While this allocation of temporary values and adjoints is inevitable,
it can be done one-time in a contiguous manner, allocating the \emph{exact} number of bytes necessary,
prior to the forward-evaluation (see Section~\ref{ssec:example}).
Moreover, this memory region can be reused for subsequent AD evaluations.
The key observation is that an expression tree 
does not need to allocate any memory for the values and adjoints;
it is only important that each expression node knows 
the number of values and adjoints it needs to view
and where they are in memory, which can be provided lazily.
Note that every node can compute the exact number of values and adjoints.
For example, any unary element-wise function on a variable of shape $m \times n$ 
will always return the same shape and size.
A more non-trivial example would be matrix multiplication.
If its arguments are $A \in \R^{m \times p}, B \in \R^{p \times n}$, then
the node's shape is a matrix of size $m \times n$.
Section~\ref{ssec:example} described our lazy allocation strategy using these observations.
The big benefit is that by design, 
since each node promises to view exactly the amount it requests, no memory is wasted.

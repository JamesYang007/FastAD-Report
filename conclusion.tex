\section{Conclusion}

In this paper, we first briefly introduced the reverse-mode automatic differentiation algorithm
to give context and background on how FastAD is implemented.
We then discussed the three motivating ideas surrounding FastAD design ---  
vectorization, lazy-evaluation, and lazy-allocation,
which make it a unique automatic differentation library.
We explored the implementation details of some of the most important expression nodes
to understand exactly how our design implements these ideas.
Additionally, we demonstrated that compile-time checks could
further optimize each individual expression node.
To see how FastAD performs in practice, we rigorously benchmarked 
a set of micro and macro-benchmarks
with other popular AD libraries and showed that FastAD consistently achieved
faster performance by orders of magnitude ranging from 2 to 20 times 
than the next fastest library depending on the problem.
We then made further analysis on the design differences,
primarily with Stan, showing that FastAD generates more vectorized code,
consumes less memory,
and allows for more inlining.

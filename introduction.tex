\section{Introduction}

Gradient computation plays a critical role in modern computing problems 
surrounding optimization, statistics, and machine learning.
For example, one may wish to compute sensitivities of 
an Ordinary-Differential-Equation (ODE) integrator
for optimization or parameter estimation~\cite{carpenter:2015}.
In Bayesian statistics, advanced Markov-Chain-Monte-Carlo (MCMC) algorithms
such as the Hamiltonian Monte Carlo (HMC) and the No-U-Turn-Sampler (NUTS) rely
heavily on computing the gradient of a (log) joint probability density function
to update proposal samples in the leapfrog algorithm~\cite{hoffman:2011}\cite{neal:2012}.
Neural networks rely on computing 
the gradient of a loss function during back-propagation
to update the weights between each layer of the network~\cite{goodfellow:2016}.

Oftentimes, the target function to differentiate is extremely complicated,
and it is very tedious and error-prone for the programmer to manually define 
the analytical formula for the gradient~\cite{margossian:2018}.
It is rather desirable to have a generic framework where the programmer 
only needs to specify the target function to differentiate it.
Moreover, computing the derivative may be one of the more expensive parts of an algorithm
if it requires numerous such evaluations, as it is the case for the three examples discussed above.
Hence, it is imperative that gradient computation is as efficient as possible.
These desires motivated the development of such a framework: \textbf{automatic differentiation} (AD).

FastAD is a general-purpose AD library in C++ supporting both forward and reverse modes of automatic differentiation.
It is highly optimized to compute gradients, but also supports computing the full Jacobian matrix.
Similar to the Stan Math Library, FastAD is primarily intended for differentiating scalar functions, 
and it is well-known that reverse-mode is more efficient than forward-mode for these cases~\cite{carpenter:2015}.
For these reasons, this paper will focus only on reverse-mode and computing gradients 
rather than forward-mode or computing a full Jacobian matrix.

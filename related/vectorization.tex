\subsection{Vectorization}

One big difference between Stan and FastAD is that 
Stan rarely generates vectorized code.
Upon closer inspection at the source code, 
this is due to how they interface with \code{Eigen}.
Unlike FastAD, Stan only creates a scalar AD variable class,
which represents a value and adjoint pair,
and uses \code{Eigen::Matrix<stan::math::var, \ldots>}
to represent a vector or matrix of AD variables.
We observed that during forward-evaluation when the coefficients must be evaluated,
Eigen methods are overloaded to return the value from the pair.
While this design reuses the scalar version,
this makes vectorization difficult because the AD variable
is now a heterogenous structure and memory access to each value
is not guaranteed to be done in a contiguous or strided fashion
(to the compiler).

To confirm this observation, we took the
summation benchmark example in Section~\ref{ssec:sum_prod}
and generated the assembly code using the compiler flags \code{-S -fverbose-asm}.
We extracted the part of the assembly that performs the summation for Stan and FastAD.\@
For brevity and formatting, 
we remove some of the comments that were generated with the assembly.
The following is the assembly for Stan:
\begin{lstlisting}[style=customasm]
L3178:
    movq    (%rax), %rdx
    addq    $8, %rax
    vaddsd  8(%rdx), %xmm0, %xmm0 
    cmpq    %rcx, %rax 
    jne L3178
\end{lstlisting}
We see that the instruction used to add is \code{vaddsd},
which is an AVX instruction to add \emph{scalar} double-precision values.
This is not a vectorized instruction, and hence addition is not done in parallel
on multiple floating point values.
Note that the prefix ``v'' in \code{vaddsd} does not mean vectorization but 
rather indicates that it is an AVX instruction.
This portion of the assembly is related to a specialization of an \code{Eigen} class 
responsible for reduction operations with \emph{default traversal},
which is no different from a naive for-loop.

Compare the above assembly with the one generated for FastAD:
\begin{lstlisting}[style=customasm]
 L3020:
     addq    $8, %rdx
     vaddpd  (%rax), %ymm1, %ymm1   
     vaddpd  32(%rax), %ymm0, %ymm0 
     addq    $64, %rax
     cmpq    %rdx, %rcx 
     jg  L3020 
\end{lstlisting}
This portion of the assembly is indeed related to the \emph{linear vectorized traversal}
specialization of the same \code{Eigen} class responsible for reduction operations.
We see that the instruction used to add is \code{vaddpd},
which is an AVX instruction to add \emph{packed} double-precision values.
This is a vectorized instruction and the operation is done in parallel
on four double-precision values.

Sometimes, Stan is able to produce vectorized code such as in matrix multiplication.
This is consistent with our benchmark results because Stan was closest to FastAD
for this case (see Section~\ref{ssec:matrix_mult}).
It is also consistent with how it is implemented,
since they allocate extra memory for \code{double} values to store the values for each matrix 
and the multiplication is carried out by viewing this region with \code{Eigen::Map} objects.
However, this vectorization does come at a cost of at least 4 times extra memory allocation.
Moreover, the backward-evaluation requires heap-allocating a matrix on-the-fly every time.
FastAD incurs no such cost, only allocates what is needed, and never heap-allocates during AD evaluation.

Adept2.0 was written to support matrix-like operation with AD.\@
Internally, all matrix operations are carried out by calling BLAS routines directly.
This explains why for most of the benchmarks, Adept was the second fastest library
because almost all of the benchmark examples required 
reduction operations such as sum, product, and exponential.
CppAD was also able vectorize some operations based on the assembly,
however there were greater costs elsewhere that hid this benefit.
Other libraries ended up using a plain for-loop with scalar instructions.

In summary, memory layout is extremely important for vectorized code.
In particular, it is very beneficial to have memory-aligned, 
contiguous region of values
and use \code{Eigen} objects with \code{double} value type for all computations.

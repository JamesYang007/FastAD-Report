\subsection{Vectorization}

One big difference between Stan and FastAD is that 
they rarely generate vectorized code.
Upon closer inspection at the source code, 
this is due to how they interface with Eigen matrix library.
Unlike FastAD, Stan only creates a scalar AD variable class,
which represents a value and adjoint pair,
and uses \code{Eigen::Matrix<stan::math::var, ...>}
to represent a vector or matrix of AD variables.
We observed that during forward-evaluation when the coefficients must be evaluated,
Eigen methods are overloaded to return just the value from the pair.
While this design reuses the scalar version,
this makes vectorization difficult because the AD variable
is now a heterogenous structure and memory access to each value
is not guaranteed to be done in a contiguous or strided fashion
(to the compiler).

To confirm this observation, we considered the
benchmark for summation as shown in Section~\ref{ssec:sum_prod}
and generated the assembly code using the compiler flags \code{-S -fverbose-asm}.
We extracted the part of the assembly that performs the summation for Stan and FastAD.\@
For brevity and formatting, 
we simplify the comments that were generated with the assembly.
The following is the assembly for Stan:
\begin{lstlisting}[style=customasm]
L3178:
# return m_functor(m_argImpl.coeff(row, col));
    movq    (%rax), %rdx    # _374->vi_, _374->vi_
# for(Index i = 1; i < mat.innerSize(); ++i)
    addq    $8, %rax    #, ivtmp.12333
# const result_type operator() 
# (const LhsScalar& a, const RhsScalar& b) const 
# { return a +  b; }
    vaddsd  8(%rdx), %xmm0, %xmm0   # MEM[(const double &)SR.7596_375 + 8], res, res
# for(Index i = 1; i < mat.innerSize(); ++i)
    cmpq    %rcx, %rax  # _1330, ivtmp.12333
    jne L3178   #,
\end{lstlisting}
We see that the instruction used to add is \code{vaddsd},
which is an AVX instruction to add \emph{scalar} double-precision values.
This is not a vectorized instruction, and hence addition is not done in parallel
on multiple floating point values.
Note that the prefix ``v'' in \code{vaddsd} does not mean vectorization but 
rather indicates that it is an AVX instruction \todo{cite Intel?}.

Compare the above assembly with the one generated for FastAD:
\begin{lstlisting}[style=customasm]
 L9109:
 # EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE 
 # void assignCoeff
 # (DstScalar& a, const SrcScalar& b) const { a += b; }
     vaddpd  (%rax), %ymm1, %ymm2    # MEM[base: _1374, offset: 0], tmp2317, vect__1661.15503
     addq    $32, %rax   #, ivtmp.15611
     vmovupd %ymm2, -32(%rax)    # vect__1661.15503, MEM[base: _1374, offset: 0]
     cmpq    %r12, %rax  # _2712, ivtmp.15611
     jne L9109   #,
\end{lstlisting}
We see that the instruction used to add is \code{vaddpd},
which is an AVX instruction to add \emph{packed} double-precision values.
This is indeed a vectorized instruction and addition is done in parallel
on four double-precision values.

Sometimes, Stan is able to produce vectorized code such as in matrix multiplication.
This is consistent with our benchmark results because Stan was closest to FastAD
for this case (see Section~\ref{ssec:matrix_mult}).
It is also consistent with how it is implemented,
since they allocate extra memory for \code{double} values to store the values for each matrix 
and the multiplication is carried out by viewing this region with \code{Eigen::Map} objects.
However, this vectorization does come at a cost of at least 4 times extra memory allocation.
Moreover, the backward-evaluation requires heap-allocating a matrix on-the-fly every time.
FastAD incurs no such cost, only allocates what is needed, and never heap-allocates during AD evaluation.

Adept2.0 was written to support matrix-like operation with AD.\@
Internally, all matrix operations are carried out by calling BLAS routines directly.
This explains why for most of the benchmarks, Adept was the second fastest library.
While other libraries ended up using a plain for-loop with scalar instructions,
Adept used vectorized code provided by BLAS.\@

The above analysis shows that memory layout is extremely important for vectorized code.
In particular, it is crucial to have memory-aligned, contiguous region of values (and adjoints)
and use \code{Eigen} matrix-like objects with \code{double} value type for all computations.
